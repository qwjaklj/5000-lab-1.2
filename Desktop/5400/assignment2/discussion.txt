(A) Download the directory from Canvas called assignment2. You will notice there are three subfolders within the data folder. One folder is called kennedy and contains works by John F. Kennedy. Similarly, the folder johnson contains Johnson’s works. The goal in this problem will be to write a Naive Bayes classifier to determine the authorship of the unlabeled works in the unlabeled folder.

(B) If you only had access to the prior probabilities, would you be more likely to guess that Kennedy or Hamilton authored an unlabeled paper? Why?
Kennedy. Because there are more texts by Kennedy than texts by Johnson.

(C) Our first step is to organize the data within these directories in some sensible way. We’ll do this using a pandas DataFrame. You will implement the missing code in build_dataframe. Specifically, follow the comments labeled “TODO” to fill in the missing code. Once you’ve finished writing the function, take a look at the data and comment on what you find. You don’t have to perform an extensive exploratory data analysis, but you have a data structure that contains text from two “authors,” so you should take some time to look through the data. Some of the questions you may want to consider are: Do one author’s documents tend to be shorter in length? What are the most common words used by each author? Do either of the authors tend to start or end their works in a consistent way? Place your answers in discussion.txt.
1. Mean lengths are 16970 of Kennedy's and 20671 of Johnson's.
2. They are ['the','of','and'] for Kennedy, ['the','to','of'] for Johnson
3. I don't see any obvious pattern there.

(D) Now, you’ve structured your data and taken some time to get a feel for each author’s works. Our next step is to complete the code for the train_nb function. Here, we will write the training step for Naive Bayes, which will incorporate Lidstone smoothing. Although, in practice, it may be uncommon for you to write an algorithm from scratch, doing so has the tremendous benefit of you translating written ideas and pseudocode into a programming language. This skill must be developed over time, and we will start here. In train_nb, the core functionality—computation of the priors and likelihoods—is missing. Follow the TODOs for guidance.

(E)What are your two prior probability estimates? What is the shape of the matrix storing your likelihoods? What happens when you vary the smoothing hyper parameter alpha?
1. P(Kennedy) = 0.647. P(Johnson) = 0.353. It estimates there are 7 texts written by Kennedy and 3 by Johnson.
2. Its (2, 22961) shape matrix
3. When we increase alpha to about 0.75, the accuracy increases. After that, when we increase alpha, the accuracy will go down.

(F)In discussion.txt , write what the predicted authors are for each of the unlabeled works.
[0, 0, 0, 1, 0, 0, 0, 1, 1, 0]
Therefore, it's Kennedy, Kennedy, Kennedy, Johnson, Kennedy, Kennedy, Kennedy, Johnson, Johnson, Kennedy

Problem 2. In this problem, we will repeat Problem 1, but this time we will use the Naive Bayes classifier from the scikit-learn library.
(A) Fill in the missing code to write the Naive Bayes classifier using scikit-learn. How do your predictions in Problem 1 compare with the scikit-learn implementation of Naive Bayes?
The prediction of scikit-learn is a little bit more accurate than my predictions in Problem1.

Problem 3. Now, we will evaluate your two models.
(A) Report the accuracy and F1-score of both the Naive Bayes classifier you created and the one off-the-shelf from scikit-learn.
Problem 1: Accuracy: 0.8  F1-score: 0.792
Problem 2: Accuracy: 0.9  F1-score: 0.792

(B) Print a confusion matrix for both the Naive Bayes classifier you created and the one off-the-shelf from scikit-learn. For each classifier, what do you notice from the confusion matrix?
There are 3 texts in the True Negative part in my prediction while there are 4 texts in scikit-learn matrix.


